{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from psycopg2 import connect\n",
    "import pandas.io.sql as pandasql\n",
    "import pandas as pd\n",
    "import configparser\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib as mpl\n",
    "import copy\n",
    "import matplotlib.dates as mdates\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "CONFIG = configparser.ConfigParser()\n",
    "CONFIG.read('C:\\\\Users\\\\rrodger\\\\db.cfg')\n",
    "dbset = CONFIG['DBSETTINGS']\n",
    "con = connect(**dbset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-2f32fcd5232a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m ORDER BY bt.analysis_id, (floor((extract('epoch' from bt.datetime_bin)-1) / 1800) * 1800))\n\u001b[0;32m     15\u001b[0m '''\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mpandasql\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_sql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp_sql\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcon\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Program Files\\Anaconda\\lib\\site-packages\\pandas\\io\\sql.py\u001b[0m in \u001b[0;36mread_sql\u001b[1;34m(sql, con, index_col, coerce_float, params, parse_dates, columns, chunksize)\u001b[0m\n\u001b[0;32m    398\u001b[0m             \u001b[0msql\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindex_col\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m             \u001b[0mcoerce_float\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcoerce_float\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparse_dates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparse_dates\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 400\u001b[1;33m             chunksize=chunksize)\n\u001b[0m\u001b[0;32m    401\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    402\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Anaconda\\lib\\site-packages\\pandas\\io\\sql.py\u001b[0m in \u001b[0;36mread_query\u001b[1;34m(self, sql, index_col, coerce_float, params, parse_dates, chunksize)\u001b[0m\n\u001b[0;32m   1438\u001b[0m         \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_convert_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1439\u001b[0m         \u001b[0mcursor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1440\u001b[1;33m         \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcol_desc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcol_desc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcursor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdescription\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1442\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "temp_sql = '''\n",
    "drop table if exists dt_30min_agg;\n",
    "create temporary table DT_30min_agg as (\n",
    "SELECT \n",
    "\tbt.analysis_id as analysis_id,\n",
    "\tTIMESTAMP WITHOUT TIME ZONE 'epoch' +\n",
    "\t\tINTERVAL '1 second' * (floor((extract('epoch' from bt.datetime_bin)-1) / 1800) * 1800) as datetime_bin,\n",
    "\tsum(bt.tt*bt.obs)/sum(bt.obs) AS travel_time,\n",
    "\tsum(bt.obs) AS obs\n",
    "FROM bluetooth.all_analyses aa\n",
    "\tINNER JOIN bluetooth.aggr_15min bt USING (analysis_id)\n",
    "WHERE left(aa.report_name, 2) = 'DT'\n",
    "GROUP BY bt.analysis_id, (floor((extract('epoch' from bt.datetime_bin)-1) / 1800) * 1800)\n",
    "ORDER BY bt.analysis_id, (floor((extract('epoch' from bt.datetime_bin)-1) / 1800) * 1800))\n",
    "'''\n",
    "pandasql.read_sql(temp_sql, con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "DatabaseError",
     "evalue": "Execution failed on sql 'select * from dt_30min_agg': relation \"dt_30min_agg\" does not exist\nLINE 1: select * from dt_30min_agg\n                      ^\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mProgrammingError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32mC:\\Program Files\\Anaconda\\lib\\site-packages\\pandas\\io\\sql.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1403\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1404\u001b[1;33m                 \u001b[0mcur\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1405\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mcur\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mProgrammingError\u001b[0m: relation \"dt_30min_agg\" does not exist\nLINE 1: select * from dt_30min_agg\n                      ^\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mDatabaseError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-b31cd6e72d92>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0msql\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'''select * from dt_30min_agg'''\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mpandasql\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_sql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcon\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Program Files\\Anaconda\\lib\\site-packages\\pandas\\io\\sql.py\u001b[0m in \u001b[0;36mread_sql\u001b[1;34m(sql, con, index_col, coerce_float, params, parse_dates, columns, chunksize)\u001b[0m\n\u001b[0;32m    398\u001b[0m             \u001b[0msql\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindex_col\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m             \u001b[0mcoerce_float\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcoerce_float\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparse_dates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparse_dates\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 400\u001b[1;33m             chunksize=chunksize)\n\u001b[0m\u001b[0;32m    401\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    402\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Anaconda\\lib\\site-packages\\pandas\\io\\sql.py\u001b[0m in \u001b[0;36mread_query\u001b[1;34m(self, sql, index_col, coerce_float, params, parse_dates, chunksize)\u001b[0m\n\u001b[0;32m   1437\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1438\u001b[0m         \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_convert_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m         \u001b[0mcursor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcol_desc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcol_desc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcursor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdescription\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Anaconda\\lib\\site-packages\\pandas\\io\\sql.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1414\u001b[0m             ex = DatabaseError(\n\u001b[0;32m   1415\u001b[0m                 \"Execution failed on sql '%s': %s\" % (args[0], exc))\n\u001b[1;32m-> 1416\u001b[1;33m             \u001b[0mraise_with_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1417\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1418\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Anaconda\\lib\\site-packages\\pandas\\compat\\__init__.py\u001b[0m in \u001b[0;36mraise_with_traceback\u001b[1;34m(exc, traceback)\u001b[0m\n\u001b[0;32m    342\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtraceback\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mEllipsis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    343\u001b[0m             \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraceback\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 344\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraceback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    345\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    346\u001b[0m     \u001b[1;31m# this version of raise is a syntax error in Python 3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Anaconda\\lib\\site-packages\\pandas\\io\\sql.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1402\u001b[0m                 \u001b[0mcur\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1403\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1404\u001b[1;33m                 \u001b[0mcur\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1405\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mcur\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1406\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDatabaseError\u001b[0m: Execution failed on sql 'select * from dt_30min_agg': relation \"dt_30min_agg\" does not exist\nLINE 1: select * from dt_30min_agg\n                      ^\n"
     ]
    }
   ],
   "source": [
    "sql = '''select * from dt_30min_agg'''\n",
    "pandasql.read_sql(sql, con)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare 15 minute buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "basql_15 = '''\n",
    "WITH bt as (\n",
    "    SELECT *\n",
    "    FROM bluetooth.aggr_15min\n",
    "    WHERE datetime_bin::date NOT BETWEEN '2017-10-15' AND '2017-10-29')\n",
    "    \n",
    "SELECT '2017-11-12'::date + datetime_bin::time as time, \n",
    "    percentile_cont(0.5) WITHIN GROUP(ORDER BY bt.tt) as travel_time,\n",
    "    CASE WHEN EXTRACT(ISODOW FROM bt.datetime_bin) < 6 THEN 'Work' ELSE 'Weekend' END as workingday,\n",
    "    aa.report_name\n",
    "\n",
    "FROM bt\n",
    "    INNER JOIN bluetooth.all_analyses aa USING(analysis_id)\n",
    "    LEFT OUTER JOIN ref.holiday hol ON (bt.datetime_bin::DATE = hol.dt)\n",
    "    \n",
    "WHERE hol.dt is NULL\n",
    "    AND left(aa.report_name, 4) = 'DT-0'\n",
    "    AND datetime_bin::date NOT BETWEEN '2017-10-15' AND '2017-10-29'\n",
    "\n",
    "GROUP BY aa.report_name, \n",
    "    datetime_bin::time, \n",
    "    CASE WHEN EXTRACT(ISODOW FROM bt.datetime_bin) < 6 THEN 'Work' ELSE 'Weekend' END\n",
    "'''\n",
    "\n",
    "travelsql_15 = '''\n",
    "SELECT bt.tt as travel_time, \n",
    "\tbt.datetime_bin, \n",
    "\tbt.analysis_id,\n",
    "\tEXTRACT(ISODOW FROM datetime_bin) as weekday,\n",
    "\taa.report_name\n",
    "    \n",
    "FROM bluetooth.aggr_15min bt\n",
    "\tINNER JOIN bluetooth.all_analyses aa ON (bt.analysis_id = aa.analysis_id)\n",
    "    LEFT OUTER JOIN ref.holiday hol ON (bt.datetime_bin::DATE = hol.dt)\n",
    "    \n",
    "WHERE hol.dt is NULL\n",
    "    AND left(aa.report_name, 4) = 'DT-0'\n",
    "    AND datetime_bin::date NOT BETWEEN '2017-10-15' AND '2017-10-29'\n",
    "'''\n",
    "baselines_15 = pandasql.read_sql(basql_15, con)\n",
    "traveltime_15 = pandasql.read_sql(travelsql_15, con)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from the fifteen minute aggregated data, these queries fetch a baseline for working and nonworking days, and the bulk travel time data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "colors = ['#003A72', '#d83904']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_base(observations, r_name):\n",
    "    #Divide data into Week and Weekend buckets for the given route name. \n",
    "    segments = {'Week' : observations[(observations['report_name'] == r_name) & \n",
    "                         (observations['workingday'] == 'Work')].sort_values(['time']),\n",
    "                'Weekend' : observations[(observations['report_name'] == r_name) & \n",
    "                         (observations['workingday'] == 'Weekend')].sort_values(['time'])}\n",
    "        \n",
    "        \n",
    "    fig, work = plt.subplots(1, 1, figsize = (16,14))\n",
    "    \n",
    "    weekend = work.twinx()\n",
    "    weekend = work.twiny()\n",
    "    days = [work, weekend]\n",
    "    \n",
    "    for i, (color, WD) in enumerate(zip(colors, ['Weekend', 'Week'])):        \n",
    "\n",
    "        days[i].plot_date(x = segments[WD].time,\n",
    "                          y = segments[WD].travel_time,\n",
    "                          xdate = True,\n",
    "                          fmt = '-o',\n",
    "                          c = color,\n",
    "                          label = WD)\n",
    "        days[i].xaxis.set_major_locator(mdates.HourLocator(interval = 3))\n",
    "        days[i].xaxis.set_major_formatter(mdates.DateFormatter('%H:%M:%S'))\n",
    "            \n",
    "    days[1].get_xaxis().set_visible(False)\n",
    "    days[1].get_yaxis().set_visible(False)\n",
    "    \n",
    "    plt.title('Baseline for ' + r_name + ' by Working Day')\n",
    "    \n",
    "    days[1].xaxis.set_label_text('Time')\n",
    "    days[1].yaxis.set_label_text('Travel Time')\n",
    "    \n",
    "    days[0].legend()\n",
    "    days[1].legend(loc = 'upper left')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "DatabaseError",
     "evalue": "Execution failed on sql '\nWITH bt as (\n    SELECT *\n    FROM king_pilot.real_tt_30min\n    WHERE bt.dt NOT BETWEEN '2017-10-15' AND '2017-10-29')\nSELECT '2017-11-12'::date + time_bin as time, \n    percentile_cont(0.5) WITHIN GROUP(ORDER BY bt.tt) as travel_time,\n    CASE WHEN EXTRACT(ISODOW FROM bt.dt) < 6 THEN 'Work' ELSE 'Weekend' END as workingday,\n    aa.report_name\n\nFROM bt\n    INNER JOIN king_pilot.bt_segments USING (bt_id)\n    INNER JOIN bluetooth.all_analyses aa USING(analysis_id)\n    LEFT OUTER JOIN ref.holiday hol ON (bt.dt::DATE = hol.dt)\n    \nWHERE hol.dt is NULL\n    AND bt.dt NOT BETWEEN '2017-10-15' AND '2017-10-29'\n\nGROUP BY aa.report_name, \n    time_bin, \n    CASE WHEN EXTRACT(ISODOW FROM bt.dt) < 6 THEN 'Work' ELSE 'Weekend' END\n': missing FROM-clause entry for table \"bt\"\nLINE 5:     WHERE bt.dt NOT BETWEEN '2017-10-15' AND '2017-10-29')\n                  ^\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mProgrammingError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32mC:\\Program Files\\Anaconda\\lib\\site-packages\\pandas\\io\\sql.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1403\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1404\u001b[1;33m                 \u001b[0mcur\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1405\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mcur\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mProgrammingError\u001b[0m: missing FROM-clause entry for table \"bt\"\nLINE 5:     WHERE bt.dt NOT BETWEEN '2017-10-15' AND '2017-10-29')\n                  ^\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mDatabaseError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-efe14726311b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     37\u001b[0m '''\n\u001b[0;32m     38\u001b[0m \u001b[1;31m#    AND datetime_bin::date NOT BETWEEN '2017-10-15' AND '2017-10-29'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m \u001b[0mbaselines_30\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpandasql\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_sql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbasql_30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcon\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[0mtraveltime_30\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpandasql\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_sql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtravelsql_30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcon\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Anaconda\\lib\\site-packages\\pandas\\io\\sql.py\u001b[0m in \u001b[0;36mread_sql\u001b[1;34m(sql, con, index_col, coerce_float, params, parse_dates, columns, chunksize)\u001b[0m\n\u001b[0;32m    398\u001b[0m             \u001b[0msql\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindex_col\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m             \u001b[0mcoerce_float\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcoerce_float\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparse_dates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparse_dates\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 400\u001b[1;33m             chunksize=chunksize)\n\u001b[0m\u001b[0;32m    401\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    402\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Anaconda\\lib\\site-packages\\pandas\\io\\sql.py\u001b[0m in \u001b[0;36mread_query\u001b[1;34m(self, sql, index_col, coerce_float, params, parse_dates, chunksize)\u001b[0m\n\u001b[0;32m   1437\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1438\u001b[0m         \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_convert_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m         \u001b[0mcursor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcol_desc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcol_desc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcursor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdescription\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Anaconda\\lib\\site-packages\\pandas\\io\\sql.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1414\u001b[0m             ex = DatabaseError(\n\u001b[0;32m   1415\u001b[0m                 \"Execution failed on sql '%s': %s\" % (args[0], exc))\n\u001b[1;32m-> 1416\u001b[1;33m             \u001b[0mraise_with_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1417\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1418\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Anaconda\\lib\\site-packages\\pandas\\compat\\__init__.py\u001b[0m in \u001b[0;36mraise_with_traceback\u001b[1;34m(exc, traceback)\u001b[0m\n\u001b[0;32m    342\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtraceback\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mEllipsis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    343\u001b[0m             \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraceback\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 344\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraceback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    345\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    346\u001b[0m     \u001b[1;31m# this version of raise is a syntax error in Python 3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Anaconda\\lib\\site-packages\\pandas\\io\\sql.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1402\u001b[0m                 \u001b[0mcur\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1403\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1404\u001b[1;33m                 \u001b[0mcur\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1405\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mcur\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1406\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDatabaseError\u001b[0m: Execution failed on sql '\nWITH bt as (\n    SELECT *\n    FROM king_pilot.real_tt_30min\n    WHERE bt.dt NOT BETWEEN '2017-10-15' AND '2017-10-29')\nSELECT '2017-11-12'::date + time_bin as time, \n    percentile_cont(0.5) WITHIN GROUP(ORDER BY bt.tt) as travel_time,\n    CASE WHEN EXTRACT(ISODOW FROM bt.dt) < 6 THEN 'Work' ELSE 'Weekend' END as workingday,\n    aa.report_name\n\nFROM bt\n    INNER JOIN king_pilot.bt_segments USING (bt_id)\n    INNER JOIN bluetooth.all_analyses aa USING(analysis_id)\n    LEFT OUTER JOIN ref.holiday hol ON (bt.dt::DATE = hol.dt)\n    \nWHERE hol.dt is NULL\n    AND bt.dt NOT BETWEEN '2017-10-15' AND '2017-10-29'\n\nGROUP BY aa.report_name, \n    time_bin, \n    CASE WHEN EXTRACT(ISODOW FROM bt.dt) < 6 THEN 'Work' ELSE 'Weekend' END\n': missing FROM-clause entry for table \"bt\"\nLINE 5:     WHERE bt.dt NOT BETWEEN '2017-10-15' AND '2017-10-29')\n                  ^\n"
     ]
    }
   ],
   "source": [
    "basql_30 = '''\n",
    "WITH bt as (\n",
    "    SELECT *\n",
    "    FROM king_pilot.real_tt_30min\n",
    "    WHERE bt.dt NOT BETWEEN '2017-10-15' AND '2017-10-29')\n",
    "SELECT '2017-11-12'::date + time_bin as time, \n",
    "    percentile_cont(0.5) WITHIN GROUP(ORDER BY bt.tt) as travel_time,\n",
    "    CASE WHEN EXTRACT(ISODOW FROM bt.dt) < 6 THEN 'Work' ELSE 'Weekend' END as workingday,\n",
    "    aa.report_name\n",
    "\n",
    "FROM bt\n",
    "    INNER JOIN king_pilot.bt_segments USING (bt_id)\n",
    "    INNER JOIN bluetooth.all_analyses aa USING(analysis_id)\n",
    "    LEFT OUTER JOIN ref.holiday hol ON (bt.dt::DATE = hol.dt)\n",
    "    \n",
    "WHERE hol.dt is NULL\n",
    "    AND bt.dt NOT BETWEEN '2017-10-15' AND '2017-10-29'\n",
    "\n",
    "GROUP BY aa.report_name, \n",
    "    time_bin, \n",
    "    CASE WHEN EXTRACT(ISODOW FROM bt.dt) < 6 THEN 'Work' ELSE 'Weekend' END\n",
    "'''\n",
    "\n",
    "travelsql_30 = '''\n",
    "SELECT bt.tt, \n",
    "\tbt.dt + bt.time_bin as datetime_bin, \n",
    "\tbt.analysis_id,\n",
    "    CASE WHEN EXTRACT(ISODOW FROM bt.dt) < 6 THEN 'Work' ELSE 'Weekend' END as workingday,\n",
    "\taa.report_name\n",
    "FROM king_pilot.real_tt_30min bt\n",
    "    INNER JOIN king_pilot.bt_segments USING (bt_id)\n",
    "\tINNER JOIN bluetooth.all_analyses aa ON (bt.analysis_id = aa.analysis_id)\n",
    "    LEFT OUTER JOIN ref.holiday hol ON (bt.dt = hol.dt)\n",
    "    \n",
    "WHERE hol.dt is NULL\n",
    "    AND datetime_bin::date NOT BETWEEN '2017-10-15' AND '2017-10-29'\n",
    "'''\n",
    "#    AND datetime_bin::date NOT BETWEEN '2017-10-15' AND '2017-10-29'\n",
    "baselines_30 = pandasql.read_sql(basql_30, con)\n",
    "traveltime_30 = pandasql.read_sql(travelsql_30, con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Fifteen minute buckets')\n",
    "plot_base(baselines_15, traveltime_30['report_name'].unique()[58])\n",
    "print('Thirty minute buckets')\n",
    "plot_base(baselines_30, traveltime_30['report_name'].unique()[58])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fifteen minute bucket baseline seems to vary rapidly, making it difficult to interpret. Changing to 30 minute aggregation makes for a much cleaner graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicsql = '''\n",
    "SELECT analysis_id as seg_id,\n",
    "    translate(right(replace(aa.report_name, ' ', ''), length(replace(aa.report_name, ' ', '')) - 8), '-_', '  ') as segment_name\n",
    "FROM bluetooth.all_analyses aa\n",
    "WHERE report_name like 'DT-0%'\n",
    "'''\n",
    "diction = pandasql.read_sql(dicsql, con)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segs = {seg: name for i, seg, name in diction.itertuples()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Makes dictionary to look up seg_id from pretty name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatterplots for given (or all) weeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "plot_weeks_sql = '''\n",
    "SELECT bt.travel_time, --y axis, as integer\n",
    "    bt.datetime_bin, --x axis as complete timestamp\n",
    "    bt.analysis_id as seg_id, --unique id\n",
    "    EXTRACT(ISODOW FROM datetime_bin) as weekday,\n",
    "    CASE WHEN EXTRACT(ISODOW FROM bt.datetime_bin) < 6 THEN 'Work' ELSE 'Weekend' END as day_type, --filter by daytype\n",
    "    --aa.report_name as segment_name, --segment title (removed in favour of naming dictionary)\n",
    "FROM dt_30min_agg bt\n",
    "    INNER JOIN bluetooth.all_analyses aa ON (bt.analysis_id = aa.analysis_id)\n",
    "    LEFT OUTER JOIN ref.holiday hol ON (bt.datetime_bin::DATE = hol.dt)\n",
    "\n",
    "WHERE hol.dt is NULL\n",
    "    AND datetime_bin::date NOT BETWEEN '2017-10-15' AND '2017-10-29'\n",
    "'''\n",
    "plot_weeks_df = pandasql.read_sql(plot_weeks_sql, con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def WOY(x, W):\n",
    "    return x.weekofyear == W\n",
    "\n",
    "def week_dict(data, seg_id): #seperate segment into individual weeks.\n",
    "    weeks = {W : data[(data['seg_id'] == seg_id) & \n",
    "                      data['datetime_bin'].apply(WOY, args = (W,))]\n",
    "            for W in data.datetime_bin.apply(lambda x : x.weekofyear).unique()}\n",
    "    \n",
    "    temp = {}\n",
    "    for week, df in weeks.items():\n",
    "        if df.travel_time.count() > 0:\n",
    "            temp[week] = weeks[week]\n",
    "        else:\n",
    "            if week in temp: #discard weeks without data to avoid MAXTICKS error\n",
    "                del temp[week]\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates a dictionary to store the bluetooth observations (30 minute) divided by week and removes empty weeks to keep the graph clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_weeks(data, seg_id):\n",
    "    weeks = week_dict(data, seg_id) #returns dictionary with seg_name divided into weeks\n",
    "        \n",
    "    fig, ax = plt.subplots(len(weeks), 1, sharex = False, sharey = True, figsize = (16, 5*len(weeks)))\n",
    "    plt.suptitle('Travel times by week for ' + segs[seg_id])\n",
    "\n",
    "    for i, week in enumerate(weeks):\n",
    "            ax[i].plot_date(x = weeks[week].datetime_bin,\n",
    "                            y = weeks[week].travel_time)\n",
    "\n",
    "            ax[i].xaxis.set_major_locator(mdates.WeekdayLocator(byweekday = [0, 1, 2, 3, 4, 5, 6])) #axis setup\n",
    "            ax[i].xaxis.set_major_formatter(mdates.DateFormatter('\\n%a %Y-%m-%d'))\n",
    "            ax[i].xaxis.set_minor_locator(mdates.HourLocator(interval = 3))\n",
    "            ax[i].xaxis.set_minor_formatter(mdates.DateFormatter('%H'))\n",
    "            \n",
    "            xpad = timedelta(minutes = 60)\n",
    "            ax[i].set_xlim(min(weeks[week].datetime_bin) - xpad, max(weeks[week].datetime_bin) + xpad)\n",
    "            \n",
    "            ax[i].set_title(str(week))# titles & labels\n",
    "            ax[i].set_xlabel('Time')\n",
    "            ax[i].set_ylabel('Travel Time')\n",
    "            ax[i].legend()\n",
    "            ax[i].xaxis.grid(True, which=\"major\")\n",
    "            ax[i].yaxis.grid(True, which=\"major\")\n",
    "\n",
    "    fig.tight_layout() #subplot titles bumping into  main title\n",
    "    \n",
    "#     if len(args) > 0:\n",
    "#         fig.subplots_adjust(top=0.88) # keep titles from getting distracted by their phones and bumping into the axis above.\n",
    "#     else:\n",
    "#         fig.subplots_adjust(top=0.965)\n",
    "    fig.subplots_adjust(top=0.965)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each segment identified in the baseline lookover, the above function will be used to first plot all weeks, then plot only the weeks with the questionable data, as identified from the first plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baselines overlaid onto percentile bands query and function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "percentile_sql = '''SELECT base.daytype as day_type,\n",
    "\t('2017-11-12 ' || base.time::varchar)::timestamp as time,\n",
    "\tbase.avg_tt as base_tt,\n",
    "\tbase.analysis_id as seg_id,\n",
    "\n",
    "\tpercentile_cont(0.1) WITHIN GROUP (ORDER BY bt.travel_time) as pct_10,\n",
    "    percentile_cont(0.2) WITHIN GROUP (ORDER BY bt.travel_time) as pct_20,\n",
    "\tpercentile_cont(0.4) WITHIN GROUP (ORDER BY bt.travel_time) as pct_40,\n",
    "    \n",
    "\tpercentile_cont(0.6) WITHIN GROUP (ORDER BY bt.travel_time) as pct_60,\n",
    "\tpercentile_cont(0.8) WITHIN GROUP (ORDER BY bt.travel_time) as pct_80,\n",
    "    percentile_cont(0.9) WITHIN GROUP (ORDER BY bt.travel_time) as pct_90,\n",
    "\tpercentile_cont(1.0) WITHIN GROUP (ORDER BY bt.travel_time) as pct_100,\n",
    "\t\n",
    "\t\n",
    "FROM king_pilot_baselines base\n",
    "\tINNER JOIN dt_30min_agg bt ON (bt.analysis_id = base.analysis_id AND bt.datetime_bin::time = base.time AND \n",
    "\t\tCASE WHEN EXTRACT(ISODOW FROM bt.datetime_bin) < 6 THEN 'weekday' ELSE 'weekend' END = base.daytype)\n",
    "\tINNER JOIN bluetooth.all_analyses aa ON (bt.analysis_id = aa.analysis_id)\n",
    "\n",
    "WHERE bt.datetime_bin::date <= '2017-11-12'\n",
    "\tAND bt.datetime_bin::date NOT BETWEEN '2017-10-15' AND '2017-10-29'\n",
    "\n",
    "GROUP BY base.analysis_id, base.time, base.daytype, base.avg_tt\n",
    "'''\n",
    "\n",
    "percentile_band = pandasql.read_sql(percentile_sql, con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentile_band[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_base(data, seg_id):\n",
    "    #Divide data into Week and Weekend buckets for the given route name. \n",
    "    segments = {'week' : data[(data['seg_id'] == seg_id) & \n",
    "                         (data['day_type'] == 'weekday')].sort_values(['time']),\n",
    "                'weekend' : data[(data['seg_id'] == seg_id) & \n",
    "                         (data['day_type'] == 'weekend')].sort_values(['time'])}\n",
    "        \n",
    "    fig, day_type = plt.subplots(2, 1, figsize = (16,14))\n",
    "    outliers = []\n",
    "    \n",
    "    for i, (color, WD) in enumerate(zip(colors, ['weekend', 'week'])):        \n",
    "        \n",
    "        day_type[i].set_title('Baseline for ' + segs[seg_id] + ' during the ' + str(WD))\n",
    "        day_type[i].plot_date(x = segments[WD].time,\n",
    "                              y = segments[WD].base_tt,\n",
    "                              xdate = True,\n",
    "                              fmt = '-o',\n",
    "                              c = color,\n",
    "                              label = WD)\n",
    "        \n",
    "        day_type[i].fill_between(segments[WD].time.values, \n",
    "                            y1=segments[WD]['pct_10'],\n",
    "                            y2=segments[WD]['pct_90'],\n",
    "                            alpha=0.15, facecolor=color)\n",
    "        day_type[i].fill_between(segments[WD].time.values, \n",
    "                            y1=segments[WD]['pct_20'],\n",
    "                            y2=segments[WD]['pct_80'],\n",
    "                            alpha=0.25, facecolor=color)\n",
    "        day_type[i].fill_between(x = segments[WD].time.values, \n",
    "                            y1=segments[WD]['pct_40'],\n",
    "                            y2=segments[WD]['pct_60'],\n",
    "                            alpha=0.35, facecolor=color)\n",
    "               \n",
    "        day_type[i].set_xlim(min(segments[WD].time) - timedelta(minutes = 30), max(segments[WD].time) + timedelta(minutes = 30))\n",
    "        day_type[i].xaxis.set_major_locator(mdates.HourLocator(byhour = range(0,24), interval = 3))\n",
    "        day_type[i].xaxis.set_major_formatter(mdates.DateFormatter('%H:%M:%S'))\n",
    "        \n",
    "        day_type[i].xaxis.set_label_text('Time')\n",
    "        day_type[i].yaxis.set_label_text('Travel Time')\n",
    "        \n",
    "        day_type[i].yaxis.grid(True)\n",
    "        day_type[i].xaxis.grid(True)\n",
    "        \n",
    "        outliers.append(daytype[i].twiny())\n",
    "        \n",
    "        outliers[i].plot_date(x = segments[WD]['time'],\n",
    "                              y = segments[WD]['pct_100'],\n",
    "                              fmt = 'x',\n",
    "                              xdate = True,\n",
    "                      #       alpha = 0.35,\n",
    "                              c = color)\n",
    "        \n",
    "        outliers[i].xaxis.set_visible(False)\n",
    "        outliers[i].set_xlim(min(segments[WD].time) - timedelta(minutes = 30), max(segments[WD].time) + timedelta(minutes = 30))\n",
    "    \n",
    "    daytype[0].legend()\n",
    "    daytype[1].legend(loc = 'upper left')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot baseline with prospective outliers removed against old baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def daystring(cut_days):\n",
    "    cut_day_str = '(\\'' + cut_days[0]\n",
    "    for day in cut_days[1:]:\n",
    "        cut_day_str = cut_day_str + '\\', \\'' + day\n",
    "    return cut_day_str + '\\')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_sql = '''WITH bt as (\n",
    "    SELECT bt.analysis_id,\n",
    "\t(TIMESTAMP WITHOUT TIME ZONE 'epoch' + INTERVAL '1 second' * (floor((extract('epoch' from bt.datetime_bin)-1) / 1800) * 1800)) as datetime_bin,\n",
    "\tsum(bt.tt*bt.obs)/sum(bt.obs) AS travel_time,\n",
    " \tsum(bt.obs) AS obs,\n",
    "    \n",
    "\n",
    "    FROM bluetooth.aggr_5min bt\n",
    "\tINNER JOIN bluetooth.all_analyses aa USING (analysis_id)\n",
    "        LEFT OUTER JOIN ref.holiday hol ON (bt.datetime_bin::date = hol.dt)\n",
    "        \n",
    "    WHERE bt.datetime_bin::date  NOT IN {0} AND\n",
    "         datetime_bin::date NOT BETWEEN '2017-10-15' AND '2017-10-29'\n",
    "        AND hol.dt is NULL\n",
    "        AND aa.analysis_id = '{1}'\n",
    "\n",
    "   GROUP BY datetime_bin, analysis_id)\n",
    "\n",
    "SELECT analysis_id as seg_id,\n",
    "    '2017-11-12'::date + datetime_bin::time as time, \n",
    "    avg(bt.travel_time) as base_tt,\n",
    "    CASE WHEN EXTRACT(ISODOW FROM bt.datetime_bin::date) < 6 THEN 'weekday' ELSE 'weekend' END as day_type\n",
    "    \n",
    "\n",
    "FROM  bt\n",
    "\n",
    "GROUP BY seg_id, \n",
    "    datetime_bin::time, \n",
    "    daytype'''\n",
    "\n",
    "\n",
    "old_sql = '''WITH bt as(\n",
    "    SELECT bt.analysis_id,\n",
    "\t(TIMESTAMP WITHOUT TIME ZONE 'epoch' + INTERVAL '1 second' * (floor((extract('epoch' from bt.datetime_bin)-1) / 1800) * 1800)) as datetime_bin,\n",
    "\tsum(bt.tt*bt.obs)/sum(bt.obs),\n",
    " \tsum(bt.obs) AS obs\n",
    "\n",
    "    FROM bluetooth.aggr_5min bt\n",
    "\tINNER JOIN bluetooth.all_analyses aa USING (analysis_id)\n",
    "        LEFT OUTER JOIN ref.holiday hol ON (bt.datetime_bin::DATE = hol.dt)\n",
    "    WHERE datetime_bin::date NOT BETWEEN '2017-10-15' AND '2017-10-29'\n",
    "        AND hol.dt is NULL\n",
    "        AND aa.analysis_id = '{0}'\n",
    "\n",
    "   GROUP BY datetime_bin, analysis_id)\n",
    "   \n",
    "SELECT analysis_id as seg_id,\n",
    "    '2017-11-12'::date + datetime_bin::time as time, \n",
    "    avg(bt.travel_time) as base_tt,\n",
    "    CASE WHEN EXTRACT(ISODOW FROM bt.datetime_bin) < 6 THEN 'weekday' ELSE 'weekend' END as day_type\n",
    "\n",
    "FROM  bt\n",
    "\n",
    "GROUP BY seg_id, \n",
    "    datetime_bin::time, \n",
    "    daytype'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The baseline query from above, modified to exclude anomalistic dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def alternate_baseline(data, seg_id, cut_day_str):\n",
    "    \n",
    "    new_base = pandasql.read_sql(new_sql.format(daystring(cut_day_str), seg_id), con)\n",
    "    old_base = pandasql.read_sql(old_sql.format(seg_id), con)\n",
    "    \n",
    "    fig, days = plt.subplots(2, 1, figsize = (16,16))\n",
    "    old = []\n",
    "\n",
    "    segments = {order : {'Work' : observations[(observations['seg_id'] == seg_id) & \n",
    "                                               (observations['daytype'] == 'weekday')].sort_values(['time']),\n",
    "                         'Weekend' : observations[(observations['seg_id'] == seg_id) & \n",
    "                                                  (observations['daytype'] == 'weekend')].sort_values(['time'])}\n",
    "                for order, observations in zip(['New', 'Old'],[new_base, old_base])}\n",
    "\n",
    "\n",
    "    for i, WD in enumerate(['Work', 'Weekend']):\n",
    "        old.append(days[i].twiny())\n",
    "        days[i].plot_date(x = segments['New'][WD].time,\n",
    "                          y = segments['New'][WD].base_tt,\n",
    "                          xdate = True,\n",
    "                          fmt = '-o',\n",
    "                          c = colors[0],\n",
    "                          alpha = 0.5,\n",
    "                          label = 'New Baseline')\n",
    "        plt.legend()\n",
    "        old[i].plot_date(x = segments['Old'][WD].time,\n",
    "                         y = segments['Old'][WD].base_tt,\n",
    "                         xdate = True,\n",
    "                         fmt = '-o',\n",
    "                         c = colors[1],\n",
    "                         alpha = 0.5,\n",
    "                         label = 'Old Baseline')\n",
    "\n",
    "        maj = mdates.HourLocator(interval = 3)\n",
    "        days[i].xaxis.set_major_locator(maj)\n",
    "        days[i].xaxis.set_major_formatter(mdates.DateFormatter('%H:%M:%S'))\n",
    "\n",
    "        #old[i].set_yticks(np.linspace(days[i].get_yticks()[0],days[i].get_yticks()[-1],len(days[i].get_yticks())))\n",
    "        old[i].get_xaxis().set_visible(False)\n",
    "        #old[i].get_yaxis().set_visible(True)\n",
    "\n",
    "        #old[i].get_yaxis().set_visible(False)\n",
    "\n",
    "\n",
    "        plt.title('Baseline for ' + segs[seg_id])\n",
    "\n",
    "        days[i].xaxis.set_label_text('Time')\n",
    "        days[i].yaxis.set_label_text('Travel Time')\n",
    "\n",
    "        days[i].legend()\n",
    "        old[i].legend(loc = 'upper left')\n",
    "\n",
    "        fig.tight_layout()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Dates affecting baselines:\n",
    "parliament NB Queen to Dundas, September 24th\n",
    "Jasrvis NB King to Queen, September 16th\n",
    "Dufferin SB Queen to King, November 5th\n",
    "Front EB Jarvis to Parliament, September 16th\n",
    "Adelaide EB Jarvis to Parliament, October 30th, 31st, November 1st. More than single point. \n",
    "Queen WB Spadina to Bathurst September 19th.\n",
    "Queen Yonge to University, Spetember 30th, October 1st, Nuit Blanche\n",
    "Queen University to Yonge, September 24th, 30th, October 1st. Nuit Blanche and Single point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The function plotting baselines returns both a weekend (blue) and weekday (orange) plot. Becasue of this not all baseline plots will be anomalistic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing October 1st has an insignificant effect on the weekend baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r_name = \"1453138\"\n",
    "plot_weeks(r_name)\n",
    "plot_base(travel_times, r_name)\n",
    "alternate_baseline(r_name, ['2017-10-01'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing October 30th has a minor effect in the early morning during the week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r_name =\"1453284\"\n",
    "plot_weeks(r_name)\n",
    "plot_base(travel_times, r_name)\n",
    "alternate_baseline(r_name, ['2017-10-30'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing September 24th, 30th, and October 1st appears to have had a very major impact on this baseline around midnight and during midday on the weekend. Nuit Blanche had a huge impact on this baseline. These dates have been removed for Queen EB Universtiy to Yonge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "r_name = \"1453627\"\n",
    "plot_weeks(r_name)\n",
    "plot_base(travel_times, r_name)\n",
    "alternate_baseline(r_name, ['2017-09-24', '2017-09-30', '2017-10-01'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing November 5th from this baseline had an insignificant impact on the new baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r_name = '1453653'\n",
    "plot_weeks(r_name)\n",
    "plot_base(travel_times, r_name)\n",
    "alternate_baseline(r_name, ['2017-11-05'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing September 30th and October 1st had a very major impact on this baseline around midnight on the weekend, due to Nuit Blanche. These dates have been removed from Queen WB Yonge to University. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "r_name = \"1453719\"\n",
    "plot_weeks(r_name)\n",
    "plot_base(travel_times, r_name)\n",
    "alternate_baseline(r_name, ['2017-09-30', '2017-10-01'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excluding September 19th had a significant impact on the late evening baseline during the week. This date has been removed from Queen WB Spadina to Bathurst. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "r_name = \"1453752\"\n",
    "plot_weeks(r_name)\n",
    "plot_base(travel_times, r_name)\n",
    "alternate_baseline(r_name, ['2017-09-19'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "October 30th, 31st, and November 1st had unique slowdowns significantly larger than any others on this segment. Removing these dates led to a very major drop in the week baseline around the PM peak. These dates have been removed from Adelaide EB Jarvis to Parliament. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "r_name = \"1454050\"\n",
    "plot_weeks(r_name)\n",
    "plot_base(travel_times, r_name)\n",
    "alternate_baseline(r_name, ['2017-10-30', '2017-10-31', '2017-11-01'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing september 24th had a noticable impact on the weekend baseline in the early morning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r_name = \"1454241\"\n",
    "plot_weeks(r_name)\n",
    "plot_base(travel_times, r_name)\n",
    "alternate_baseline(r_name, ['2017-09-24'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Removing September 16th from this baseline had a major impact on the peak hour during the weekend. This date has been removed from Front EB Jarvis to Parliament. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "r_name = \"1454605\"\n",
    "plot_weeks(r_name)\n",
    "plot_base(travel_times, r_name)\n",
    "alternate_baseline(r_name, ['2017-09-16'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By removing November 5th, the weekend baseline changed in a major way around peak PM hour. This date has been removed from Dufferin SB Queen to King. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "r_name = \"1454879\"\n",
    "plot_weeks(r_name)\n",
    "plot_base(travel_times, r_name)\n",
    "alternate_baseline(r_name, ['2017-11-05'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing November 5th had a notable impact in the early and mid morning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r_name = \"1454907\"\n",
    "plot_weeks(r_name)\n",
    "plot_base(travel_times, r_name)\n",
    "alternate_baseline(r_name, ['2017-11-05'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This outlier was minor and didn't affect the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r_name = \"1454997\"\n",
    "plot_weeks(r_name)\n",
    "plot_base(travel_times, r_name)\n",
    "alternate_baseline(r_name, ['2017-10-01'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing October 14th had a significant impact on the early morning of the weekend baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r_name = \"1455076\"\n",
    "plot_weeks(r_name)\n",
    "plot_base(travel_times, r_name)\n",
    "alternate_baseline(r_name, ['2017-10-14'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing November 4th had a significant impact on the early morning, morning, and evening weekend baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r_name = \"1455088\"\n",
    "plot_weeks(r_name)\n",
    "plot_base(travel_times, r_name)\n",
    "alternate_baseline(r_name, ['2017-11-04'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing September 19th had a noticable impact on the weekday  baseline in the evening and at midday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r_name = \"1455231\"\n",
    "plot_weeks(r_name)\n",
    "plot_base(travel_times, r_name)\n",
    "alternate_baseline(r_name, ['2017-09-19'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing November 5th had a significant effect on the weekend baseline early in the morning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r_name = \"1455243\"\n",
    "plot_weeks(r_name)\n",
    "plot_base(travel_times, r_name)\n",
    "alternate_baseline(r_name, ['2017-11-05'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing October and November 8th resulted in insignificant changes to the weekend baseline in the early morning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r_name = \"1455351\"\n",
    "plot_weeks(r_name)\n",
    "plot_base(travel_times, r_name)\n",
    "alternate_baseline(r_name, ['2017-10-08', '2017-11-08'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing September 16th had a notable impact on the baseline during midday. This date has been removed from Jarvis NB Front to King. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "r_name = \"1455538\"\n",
    "plot_weeks(r_name)\n",
    "plot_base(travel_times, r_name)\n",
    "alternate_baseline(r_name, ['2017-09-16'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing September 17th had a significant impact on the weekend baseline during midday. This date has been removed from Jarvis NB King to Queen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "r_name = \"1455555\"\n",
    "plot_weeks(r_name)\n",
    "plot_base(travel_times, r_name)\n",
    "alternate_baseline(r_name, ['2017-09-17'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing September 16th had a major impact on the weekend baseline early in the morning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r_name = \"1455628\"\n",
    "plot_weeks(r_name)\n",
    "plot_base(travel_times, r_name)\n",
    "alternate_baseline(r_name, ['2017-09-16'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing September 16th had a major impact on the weekend baseline in the morning. This date has been removed from Parliament NB Queen to Dundas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "r_name = \"1455676\"\n",
    "plot_weeks(r_name)\n",
    "plot_base(travel_times, r_name)\n",
    "alternate_baseline(r_name, ['2017-09-24'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the segments with outliers are listed sinnister segments below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sinnister_segments = [\"DT-0001.College-EB_Bathurst-to-University\",\n",
    "\"DT-0007. Dundas-EB_Bathurst-to-Spadina\",\n",
    "\"DT-0024. Queen-EB_University-to-Yonge\",\n",
    "\"DT-0026. Queen-EB_Jarvis-to-Parliament\",\n",
    "\"DT-0031. Queen-WB_Yonge-to-University\",\n",
    "\"DT-0033. Queen-WB_Spadina-to-Bathurst\",\n",
    "\"DT-0047. Adelaide-EB_Jarvis-to-Parliament\",\n",
    "\"DT-0056. King-EB_Jarvis-to-Parliament\",\n",
    "\"DT-0074. Front-EB_Jarvis-to-Parliament\",\n",
    "\"DT-0086. Dufferin-SB_Queen-to-King\",\n",
    "\"DT-0088. Dufferin-NB_Queen-to-Dundas\",\n",
    "\"DT-0094. Bathurst-SB_King-to-Front\",\n",
    "\"DT-0099. Bathurst-NB_Queen-to-Dundas\",\n",
    "\"DT-0100. Bathurst-NB_Dundas-to-College\",\n",
    "\"DT-0108. Spadina-NB_Queen-to-Dundas\",\n",
    "\"DT-0109. University-SB_College-to-Dundas\",\n",
    "\"DT-0115. University-NB_Front-to-King\",\n",
    "\"DT-0128. Jarvis-NB_Front-to-King\",\n",
    "\"DT-0129. Jarvis-NB_King-to-Queen\",\n",
    "\"DT-0134. Parliament-SB_King-to-Front\",\n",
    "\"DT-0137. Parliament-NB_Queen-to-Dundas\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All dates removed are listed below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "removed_dates = [\n",
    "    [\"DT-0024. Queen-EB_University-to-Yonge\", '2017-09-24', '2017-09-30', '2017-10-01'],\n",
    "    [\"DT-0031. Queen-WB_Yonge-to-University\", '2017-09-30', '2017-10-01'],\n",
    "    [\"DT-0033. Queen-WB_Spadina-to-Bathurst\", '2017-09-19'],\n",
    "    [\"DT-0047. Adelaide-EB_Jarvis-to-Parliament\", '2017-10-30', '2017-10-31', '2017-11-01'],\n",
    "    [\"DT-0074. Front-EB_Jarvis-to-Parliament\", '2017-09-16'],\n",
    "    [\"DT-0086. Dufferin-SB_Queen-to-King\", '2017-11-05'],\n",
    "    [\"DT-0128. Jarvis-NB_Front-to-King\", '2017-09-16'],\n",
    "    [\"DT-0129. Jarvis-NB_King-to-Queen\", '2017-09-17'],\n",
    "    [\"DT-0137. Parliament-NB_Queen-to-Dundas\", '2017-09-24'],\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After looking at single points with highly inflated travel times and longer periods where the baseline was affected by an anomalistic slowdown, it doesn't look like removing either of these types of anomalies has a very controllable impact on the baselines. Since the baseline data is so limited, removing days can change the part of the baseline that wasn't affected by the anomaly as much or even more than the baseline at the time of the anomaly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
